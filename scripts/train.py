import torch
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np

# ================= æ–°å¢ï¼šå¯¹æ¯”æŸå¤±å‡½æ•° =================
def physics_contrast_loss(attentions, adj_mask, conflict_mask=None):
    """
    ç‰©ç†å¯¹æ¯”æŸå¤± - å®Œå…¨é‡å†™ç‰ˆ
    
    ç›®æ ‡ï¼š
    1. ç‰©ç†è¾¹çš„æƒé‡åº”è¯¥ > 0.3
    2. éç‰©ç†è¾¹çš„æƒé‡åº”è¯¥ < 0.1
    3. å†²çªæ—¶ï¼Œæ ‡è®°çš„è¾¹æƒé‡åº”è¯¥ < 0.1
    """
    B, N, _ = attentions.shape
    device = attentions.device
    
    if conflict_mask is None:
        # === æ­£å¸¸æ ·æœ¬ ===
        # æ„å»ºç‰©ç†è¾¹æ©ç ï¼ˆæ’é™¤è‡ªè¿æ¥ï¼‰
        eye_mask = torch.eye(N).to(device)
        phy_edges = (adj_mask > 0) & (eye_mask == 0)
        non_phy_edges = (adj_mask == 0)
        
        # Loss 1: ç‰©ç†è¾¹æƒé‡ä¸è¶³çš„æƒ©ç½š
        # æœŸæœ›æ¯æ¡ç‰©ç†è¾¹çš„æƒé‡ > 0.3
        phy_weights = attentions * phy_edges.unsqueeze(0)  # (B, N, N)
        phy_violations = torch.relu(0.3 - phy_weights)  # ä½äº0.3å°±æƒ©ç½š
        loss_phy = phy_violations.sum() / (phy_edges.sum() * B + 1e-6)
        
        # Loss 2: éç‰©ç†è¾¹æƒé‡è¿‡é«˜çš„æƒ©ç½š
        # æœŸæœ›æ¯æ¡éç‰©ç†è¾¹çš„æƒé‡ < 0.1
        non_phy_weights = attentions * non_phy_edges.unsqueeze(0)
        non_phy_violations = torch.relu(non_phy_weights - 0.1)  # é«˜äº0.1å°±æƒ©ç½š
        loss_non_phy = non_phy_violations.sum() / (non_phy_edges.sum() * B + 1e-6)
        
        total_loss = loss_phy + loss_non_phy
        
    else:
        # === å†²çªæ ·æœ¬ ===
        # æ ‡è®°ä¸ºå†²çªçš„è¾¹æƒé‡åº”è¯¥ < 0.1
        conflict_weights = attentions * conflict_mask
        violations = torch.relu(conflict_weights - 0.1)
        total_loss = violations.sum() / (conflict_mask.sum() + 1e-6)
    
    return total_loss


def create_conflict_batch(x_batch, conflict_ratio=0.5):
    """
    äººå·¥åˆ¶é€ ç‰©ç†å†²çªæ ·æœ¬ - å¢å¼ºç‰ˆ
    
    ç­–ç•¥1ï¼šéšæœºäº¤æ¢ Accel æ•°æ®ï¼ˆç ´å Accel-Baro ä¸€è‡´æ€§ï¼‰
    ç­–ç•¥2ï¼šéšæœºäº¤æ¢ Gyro æ•°æ®ï¼ˆç ´å Gyro-Mag ä¸€è‡´æ€§ï¼‰
    
    è¿”å›:
    - x_conflict: å†²çªæ ·æœ¬
    - conflict_mask: (B, N, N) æ ‡è®°å“ªäº›è¾¹æ˜¯å†²çªçš„
    """
    B, T, N, F = x_batch.shape
    x_conflict = x_batch.clone()
    conflict_mask = torch.zeros(B, N, N).to(x_batch.device)
    
    # é€‰æ‹©æ›´å¤šæ ·æœ¬è¿›è¡Œç ´åï¼ˆä»30%æå‡åˆ°50%ï¼‰
    n_conflict = int(B * conflict_ratio)
    conflict_indices = torch.randperm(B)[:n_conflict]
    
    for idx in conflict_indices:
        # éšæœºé€‰æ‹©ç ´åç­–ç•¥
        strategy = torch.rand(1).item()
        
        if strategy < 0.5:
            # ç­–ç•¥1ï¼šäº¤æ¢ Accel (Node 1)
            swap_idx = torch.randint(0, B, (1,)).item()
            if swap_idx == idx:
                swap_idx = (idx + 1) % B
            
            x_conflict[idx, :, 1, :] = x_batch[swap_idx, :, 1, :]
            # æ ‡è®°å†²çªè¾¹
            conflict_mask[idx, 3, 1] = 1.0  # Accel -> Baro
            conflict_mask[idx, 4, 1] = 1.0  # Accel -> GPS
        else:
            # ç­–ç•¥2ï¼šäº¤æ¢ Gyro (Node 2)
            swap_idx = torch.randint(0, B, (1,)).item()
            if swap_idx == idx:
                swap_idx = (idx + 1) % B
            
            x_conflict[idx, :, 2, :] = x_batch[swap_idx, :, 2, :]
            # æ ‡è®°å†²çªè¾¹
            conflict_mask[idx, 5, 2] = 1.0  # Gyro -> Mag
            conflict_mask[idx, 1, 2] = 1.0  # Gyro -> Accel
    
    return x_conflict, conflict_mask


# ================= ä¿®æ”¹åçš„è®­ç»ƒå‡½æ•° =================
def heteroscedastic_loss(true, mean, log_var):
    """
    é«˜æ–¯è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤± - æ•°å€¼ç¨³å®šç‰ˆ
    å¼ºåˆ¶ sigma ä¸èƒ½è¿‡å°
    """
    # é™åˆ¶ log_var çš„èŒƒå›´ï¼Œé˜²æ­¢ sigma å´©å¡Œåˆ°0
    # log_var èŒƒå›´: [-2, 2] å¯¹åº” sigma èŒƒå›´: [0.37, 2.72]
    log_var = torch.clamp(log_var, min=-2, max=2)
    
    precision = torch.exp(-log_var)
    mse = (true - mean) ** 2
    loss = 0.5 * precision * mse + 0.5 * log_var
    
    # ç¡®ä¿æŸå¤±éè´Ÿ
    loss = torch.clamp(loss, min=0)
    return loss.mean()


def train_with_contrast(model, train_loader, test_loader, adj_mask, 
                       epochs=100, lr=1e-3, lambda_contrast=0.5, device='cuda'):
    """
    å¸¦å¯¹æ¯”å­¦ä¹ çš„è®­ç»ƒæµç¨‹
    
    lambda_contrast: å¯¹æ¯”æŸå¤±çš„æƒé‡ï¼ˆæå‡åˆ°0.5ï¼‰
    """
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-2)  # å¢å¼ºåˆ°1e-2
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)
    best_val_loss = float('inf')
    train_history = []
    val_history = []
    patience_counter = 0
    early_stop_patience = 20  # 20ä¸ªepochéªŒè¯é›†ä¸é™å°±åœ
    
    for epoch in range(epochs):
        model.train()
        total_nll = 0
        total_contrast = 0
        
        for x_batch, y_batch in train_loader:
            x_batch = x_batch.to(device)
            y_batch = y_batch.to(device)
            
            # === æ­£å¸¸æ ·æœ¬å‰å‘ä¼ æ’­ ===
            mu, log_var, attn = model(x_batch, adj_mask, return_all_steps=False)
            loss_nll = heteroscedastic_loss(y_batch, mu, log_var)
            
            # === å¯¹æ¯”å­¦ä¹ ï¼šæ­£å¸¸æ ·æœ¬ ===
            loss_contrast_normal = physics_contrast_loss(attn, adj_mask)
            
            # === å¯¹æ¯”å­¦ä¹ ï¼šå†²çªæ ·æœ¬ ===
            x_conflict, conflict_mask = create_conflict_batch(x_batch, conflict_ratio=0.3)
            _, _, attn_conflict = model(x_conflict, adj_mask, return_all_steps=False)
            loss_contrast_conflict = physics_contrast_loss(attn_conflict, adj_mask, conflict_mask)
            
            # === æ€»æŸå¤± ===
            loss_total = loss_nll + lambda_contrast * (loss_contrast_normal + loss_contrast_conflict)
            
            optimizer.zero_grad()
            loss_total.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            total_nll += loss_nll.item()
            total_contrast += (loss_contrast_normal + loss_contrast_conflict).item()
        
        avg_nll = total_nll / len(train_loader)
        avg_contrast = total_contrast / len(train_loader)
        
        # === éªŒè¯ ===
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for x_batch, y_batch in test_loader:
                x_batch = x_batch.to(device)
                y_batch = y_batch.to(device)
                mu, log_var, _ = model(x_batch, adj_mask)
                loss = heteroscedastic_loss(y_batch, mu, log_var)
                total_val_loss += loss.item()
        
        avg_val_loss = total_val_loss / len(test_loader)
        
        train_history.append(avg_nll)
        val_history.append(avg_val_loss)
        
        print(f"Epoch [{epoch+1}/{epochs}] "
              f"NLL: {avg_nll:.4f} | Contrast: {avg_contrast:.4f} | Val: {avg_val_loss:.4f}")
        
        # === æ·»åŠ å¼‚å¸¸æ£€æµ‹ ===
        if avg_nll < 0 or avg_contrast < 0:
            print(f"  âš ï¸ WARNING: Negative loss detected! This indicates numerical instability.")
        
        # ç›‘æ§å¯¹æ¯”å­¦ä¹ çš„æ•ˆæœ
        if epoch == 0:
            print(f"  ğŸ“Š First epoch - Contrast Loss: {avg_contrast:.4f}")
        if epoch == 20:
            print(f"  ğŸ“Š Epoch 20 - Contrast Loss should be decreasing")
            if avg_contrast > 1.0:
                print(f"  âš ï¸ Contrast Loss still high! Model may not be learning edge patterns.")
        
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            torch.save(model.state_dict(), 'best_model_contrast.pth')
            print(f"  â†’ Model saved! (Val improved)")
        else:
            patience_counter += 1
            if patience_counter >= early_stop_patience:
                print(f"\nâš ï¸ Early stopping at epoch {epoch+1}")
                break
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(avg_val_loss)
    
    print(f"Training Complete. Best Val Loss: {best_val_loss:.4f}")
    
    # === ä¿å­˜è®­ç»ƒæ›²çº¿ ===
    import matplotlib.pyplot as plt
    plt.figure(figsize=(10, 5))
    plt.plot(train_history, label='Train NLL')
    plt.plot(val_history, label='Val Loss')
    plt.title('PhyGAT Training with Contrast Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig('training_curve.png')
    print("Saved training curve to 'training_curve.png'")


# ================= ä½¿ç”¨ç¤ºä¾‹ =================
if __name__ == "__main__":
    from dataset_build import FlightDataset, get_adjacency_matrix
    from model import PhyGAT_Fixed
    
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åŠ è½½æ•°æ®
    train_ds = FlightDataset('dataset/flight_dataset.npy', mode='train')
    test_ds = FlightDataset('dataset/flight_dataset.npy', mode='test')
    
    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)
    
    # åŠ è½½é‚»æ¥çŸ©é˜µ
    adj_matrix = get_adjacency_matrix().to(DEVICE)
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = PhyGAT_Fixed(num_nodes=6, in_dim=3).to(DEVICE)
    
    # è®­ç»ƒ
    train_with_contrast(
        model, train_loader, test_loader, adj_matrix,
        epochs=100, lr=1e-3, lambda_contrast=0.1, device=DEVICE
    )